{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "# use model \"chinese_roberta_wwm_ext_pytorch\"\n",
    "import torch\n",
    "import time \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import *\n",
    "\n",
    "path = \"data/\"\n",
    "bert_path = \"chinese_roberta_wwm_ext_pytorch/\"\n",
    "tokenizer = BertTokenizer(vocab_file=bert_path + \"vocab.txt\")  # 初始化分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "8679it [00:00, 12809.71it/s]\n"
    }
   ],
   "source": [
    "input_ids = []     # input char ids\n",
    "input_types = []   # segment ids\n",
    "input_masks = []   # attention mask\n",
    "label = []         # 标签\n",
    "pad_size = 20     # 也称为 max_len (前期统计分析，文本长度最大值为38，取32即可覆盖99%)\n",
    " \n",
    "with open(path + \"train.txt\", encoding='utf-8') as f:\n",
    "    for i, l in tqdm(enumerate(f)): \n",
    "        x1, y = l.strip().split('\\t')\n",
    "        x1 = tokenizer.tokenize(x1)\n",
    "        tokens = [\"[CLS]\"] + x1 + [\"[SEP]\"]\n",
    "        \n",
    "        # 得到input_id, seg_id, att_mask\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        types = [0] *(len(ids))\n",
    "        masks = [1] * len(ids)\n",
    "        # 短则补齐，长则切断\n",
    "        if len(ids) < pad_size:\n",
    "            types = types + [1] * (pad_size - len(ids))  # mask部分 segment置为1\n",
    "            masks = masks + [0] * (pad_size - len(ids))\n",
    "            ids = ids + [0] * (pad_size - len(ids))\n",
    "        else:\n",
    "            types = types[:pad_size]\n",
    "            masks = masks[:pad_size]\n",
    "            ids = ids[:pad_size]\n",
    "        input_ids.append(ids)\n",
    "        input_types.append(types)\n",
    "        input_masks.append(masks)\n",
    "#         print(len(ids), len(masks), len(types)) \n",
    "        assert len(ids) == len(masks) == len(types) == pad_size\n",
    "        label.append([int(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2420, 7581, 7880, 6782, 1290, 3345, 3788, 1280, 7217, 5058]\n(6943, 20) (6943, 20) (6943, 20) (6943, 1)\n(1736, 20) (1736, 20) (1736, 20) (1736, 1)\n"
    }
   ],
   "source": [
    "# 随机打乱索引\n",
    "random_order = list(range(len(input_ids)))\n",
    "np.random.seed(2020)   # 固定种子\n",
    "np.random.shuffle(random_order)\n",
    "print(random_order[:10])\n",
    "\n",
    "# 4:1 划分训练集和测试集\n",
    "input_ids_train = np.array([input_ids[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_types_train = np.array([input_types[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_masks_train = np.array([input_masks[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "y_train = np.array([label[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "print(input_ids_train.shape, input_types_train.shape, input_masks_train.shape, y_train.shape)\n",
    "\n",
    "input_ids_test = np.array([input_ids[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_types_test = np.array([input_types[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_masks_test = np.array([input_masks[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "y_test = np.array([label[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "print(input_ids_test.shape, input_types_test.shape, input_masks_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
    "                           torch.LongTensor(input_types_train), \n",
    "                           torch.LongTensor(input_masks_train), \n",
    "                           torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
    "                          torch.LongTensor(input_types_test), \n",
    "                         torch.LongTensor(input_masks_test),\n",
    "                          torch.LongTensor(y_test))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path)  # /bert_pretrain/\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  # 每个参数都要 求梯度 feiyu 改成false\n",
    "        self.fc = nn.Linear(768, 10)   # 768 -> 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子   (ids, seq_len, mask)\n",
    "        types = x[1]\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子相同size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, token_type_ids=types, \n",
    "                              attention_mask=mask, \n",
    "                              output_all_encoded_layers=False) # 控制是否输出所有encoder层的结果\n",
    "        out = self.fc(pooled)   # 得到10分类\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda\nModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (fc): Linear(in_features=768, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'\n",
    "print(DEVICE)\n",
    "model = Model().to(DEVICE)\n",
    "\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())  # 模型参数名字列表\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "NUM_EPOCHS = 3\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-4,\n",
    "                     warmup=0.05,\n",
    "                     t_total=len(train_loader) * NUM_EPOCHS\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    best_acc = 0.0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        y_pred = model([x1, x2, x3])  # 得到预测结果\n",
    "        model.zero_grad()             # 梯度清零\n",
    "        loss = F.cross_entropy(y_pred, y.squeeze())  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 100 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(epoch, (batch_idx+1) * len(x1), \n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader), \n",
    "                                                                           loss.item()))  # 记得为loss.item()\n",
    "\n",
    "def test(model, device, test_loader):    # 测试模型, 得到测试集评估结果\n",
    "    model.eval()\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(test_loader):\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1,x2,x3])\n",
    "        test_loss += F.cross_entropy(y_, y.squeeze())\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max(): 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "          test_loss, acc, len(test_loader.dataset),\n",
    "          100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 1 [800/6943 (11.41%)]\tLoss: 0.778597\nTrain Epoch: 1 [1600/6943 (22.93%)]\tLoss: 0.621922\nTrain Epoch: 1 [2400/6943 (34.45%)]\tLoss: 1.100434\nTrain Epoch: 1 [3200/6943 (45.97%)]\tLoss: 0.930870\nTrain Epoch: 1 [4000/6943 (57.49%)]\tLoss: 0.684451\nTrain Epoch: 1 [4800/6943 (69.01%)]\tLoss: 0.737182\nTrain Epoch: 1 [5600/6943 (80.53%)]\tLoss: 0.920716\nTrain Epoch: 1 [6400/6943 (92.05%)]\tLoss: 0.747857\n\nTest set: Average loss: 0.7864, Accuracy: 1194/1736 (68.78%)\nacc is: 0.6878, best acc is 0.6878\n\nTrain Epoch: 2 [800/6943 (11.41%)]\tLoss: 0.716462\nTrain Epoch: 2 [1600/6943 (22.93%)]\tLoss: 0.749733\nTrain Epoch: 2 [2400/6943 (34.45%)]\tLoss: 0.680745\nTrain Epoch: 2 [3200/6943 (45.97%)]\tLoss: 0.700463\nTrain Epoch: 2 [4000/6943 (57.49%)]\tLoss: 0.662157\nTrain Epoch: 2 [4800/6943 (69.01%)]\tLoss: 0.979742\nTrain Epoch: 2 [5600/6943 (80.53%)]\tLoss: 0.561021\nTrain Epoch: 2 [6400/6943 (92.05%)]\tLoss: 0.793773\n\nTest set: Average loss: 0.7488, Accuracy: 1219/1736 (70.22%)\nacc is: 0.7022, best acc is 0.7022\n\nTrain Epoch: 3 [800/6943 (11.41%)]\tLoss: 0.669762\nTrain Epoch: 3 [1600/6943 (22.93%)]\tLoss: 0.515458\nTrain Epoch: 3 [2400/6943 (34.45%)]\tLoss: 0.776434\nTrain Epoch: 3 [3200/6943 (45.97%)]\tLoss: 0.784594\nTrain Epoch: 3 [4000/6943 (57.49%)]\tLoss: 0.800167\nTrain Epoch: 3 [4800/6943 (69.01%)]\tLoss: 0.782289\nTrain Epoch: 3 [5600/6943 (80.53%)]\tLoss: 0.600493\nTrain Epoch: 3 [6400/6943 (92.05%)]\tLoss: 0.599682\n\nTest set: Average loss: 0.7416, Accuracy: 1226/1736 (70.62%)\nacc is: 0.7062, best acc is 0.7062\n\n"
    }
   ],
   "source": [
    "best_acc = 0.0 \n",
    "PATH = 'roberta_model.pth'  # 定义模型保存路径\n",
    "for epoch in range(1, NUM_EPOCHS+1):  # 3个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)  # 保存最优模型\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTest set: Average loss: 0.7416, Accuracy: 1226/1736 (70.62%)\nacc is: 0.7062, best acc is 0.7062\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nPATH = \"roberta_model.pth\"\\nmodel.load_state_dict(torch.load(PATH))\\ndef test_for_submit(model, device, test_loader):    # 测试模型\\n    model.eval()\\n    preds = []\\n    for batch_idx, (x1,x2,x3) in tqdm(enumerate(test_loader)):\\n        x1,x2,x3 = x1.to(device), x2.to(device), x3.to(device)\\n        with torch.no_grad():\\n            y_ = model([x1,x2,x3])\\n        pred = y_.max(-1, keepdim=True)[1].squeeze().cpu().tolist()   \\n        # .max() 2输出，分别为最大值和最大值的index\\n        preds.extend(pred) \\n    return preds \\npreds = test_for_submit(model, DEVICE, test_loader)\\n'"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"roberta_model.pth\"))\n",
    "acc = test(model, DEVICE, test_loader)\n",
    "if best_acc < acc: \n",
    "    best_acc = acc \n",
    "    torch.save(model.state_dict(), PATH)  # 保存最优模型\n",
    "print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc))  \n",
    "\n",
    "\n",
    "# 测试集提交\n",
    "'''\n",
    "PATH = \"roberta_model.pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "def test_for_submit(model, device, test_loader):    # 测试模型\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for batch_idx, (x1,x2,x3) in tqdm(enumerate(test_loader)):\n",
    "        x1,x2,x3 = x1.to(device), x2.to(device), x3.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1,x2,x3])\n",
    "        pred = y_.max(-1, keepdim=True)[1].squeeze().cpu().tolist()   \n",
    "        # .max() 2输出，分别为最大值和最大值的index\n",
    "        preds.extend(pred) \n",
    "    return preds \n",
    "preds = test_for_submit(model, DEVICE, test_loader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}